---
layout: default # Or a specific 'post' layout if you create one later
title: "Is safe AI attainable?"
date: 2025-07-23 15:00:00 +0100 # Date and time of publication
author: "Suyash Agarwal" # Optional
published: false
categories: [research, AI, safety] # Optional
tags: [] # Optional
---

## On the safety of AI systems

It seems clear that AI is here to stay. If this is not clear to you, consider that 78% of respondents to a recent McKinsey survey said their organisation uses AI in at least one business function, across a wide variety of regions, industries and company sizes. AI companies also received 42% of all US venture capital investment in 2024 and all projections indicate that this is not a passing trend. 

So given that AI is here to stay, there are a few questions that spring to mind:
- To what extent is AI going to change our lives in the foreseeable future?
- What are the risks that AI poses to our way of life?
- Is there anything we can do about it?

I'm going to focus on the last question in this piece. The first two questions have been discussed at length by a myriad of very knowledgeable people, and rightly so. But I think the last question is non-trivial and has maybe received relatively little attention. I also want to focus on this because my first instinct was that AI safety research would be difficult almost to the point of being futile. How can we possibly hope to control the vast majority of behaviours that we expect out of an LLM? And perhaps more importantly, how can we control what people do with this technology as it proliferates?

### The risks

Although I said I would focus on the last question, it's hard to answer that without briefly touching on the risks posed by AI. Here's a brief non-exhaustive list of some scenarios, ~~some~~ most of which are already playing out:
- jobs get replaced by AI en masse, leading to high unemployment and income inequality
- people stop thinking critically altogether as a result of outsourcing more and more cognitive tasks to AI (see this [recent study by MIT](https://arxiv.org/abs/2506.08872))
- malicious actors use AI to influence elections or spread propaganda
- the internet gets filled up with AI slop and/or misinformation, making accurate high-quality information difficult to find or identify
- creative industries fail as the arts get undervalued by the proliferation of AI-generated art, music and text

I'm sure I will have missed some things off this list but these seem to be some of the key immediate risks to society. I'm deliberately avoiding calling out the 'existential' risk of superintelligent AI, as it's a subject of debate and feels a little speculative at the moment. These are real issues that are already happening. 

### The disconnect 

The way I see it, there is a disconnect between existing AI research and the risks outlined above. Existing research tackles problems such as alignment, interpretability, scalable oversight and robustness to adversarial/anomalous inputs. 

But the real issue is deeper - we have to plan for a future where anyone can train their own LLM to do whatever they want it to. Studying model alignment is only useful as long as the individuals and companies training AI models commit to aligning their models with 'good' human values. 


However, there are more practical reasons why current methods may not be sufficient. Current interpretability methods struggle to scale up to the largest, most capable models. This is a big problem - we can learn a lot from model organisms but the biggest risks are posed by the largest models via behaviours that smaller models cannot replicate.
- Moving away from the technical challenge of controlling an AI system, perhaps the biggest risk is posed by what _people_ can do once they get their hands on powerful open-source LLMs. Any alignment research then goes out the window - we could have a perfect recipe for alignment but it's only useful if the person training their model refers to it. To me, this is the biggest reason why current research efforts, while very impressive, are simply not enough to address the risks posed by AI. 

I should say at this point that I completely understand why AI safety research has gone in the direction of controlling AI outputs and understanding how models work. These are technical, definable questions to ask and we can use research techniques from machine learning (or in some cases, neuroscience) to try to answer them. I also don't mean to minimse the fascinating research that is being done already. I think Anthropic's recent paper on [tracing the thoughts of a large language model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) was one of the most interesting papers I've ever read, especially as someone caught between the worlds of deep learning and neuroscience. But in the next section I'll discuss why I don't think this is even close to being sufficient.







